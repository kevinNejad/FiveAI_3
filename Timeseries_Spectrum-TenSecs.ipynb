{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.4f' %x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context('paper', font_scale=1.3)\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn import preprocessing\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>DateTime</th>\n",
       "      <th>Second</th>\n",
       "      <th>Minute</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Day</th>\n",
       "      <th>weekday</th>\n",
       "      <th>YYYY</th>\n",
       "      <th>MM</th>\n",
       "      <th>DD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>2007-05-01 19:55:56.762087</td>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>19</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>2007-05-01 19:55:56.890691</td>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>19</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>2007-05-01 19:55:56.890840</td>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>19</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>178</td>\n",
       "      <td>2007-05-01 19:55:56.891191</td>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>19</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52</td>\n",
       "      <td>2007-05-01 19:55:57.019846</td>\n",
       "      <td>57</td>\n",
       "      <td>55</td>\n",
       "      <td>19</td>\n",
       "      <td>Weekday</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Length                   DateTime  Second  Minute  Hour      Day  weekday  \\\n",
       "0      60 2007-05-01 19:55:56.762087      56      55    19  Weekday        1   \n",
       "1      60 2007-05-01 19:55:56.890691      56      55    19  Weekday        1   \n",
       "2      52 2007-05-01 19:55:56.890840      56      55    19  Weekday        1   \n",
       "3     178 2007-05-01 19:55:56.891191      56      55    19  Weekday        1   \n",
       "4      52 2007-05-01 19:55:57.019846      57      55    19  Weekday        1   \n",
       "\n",
       "   YYYY  MM  DD  \n",
       "0  2007   5   1  \n",
       "1  2007   5   1  \n",
       "2  2007   5   1  \n",
       "3  2007   5   1  \n",
       "4  2007   5   1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headers = pd.read_pickle('./Dataset/May01_09_new.pkl')\n",
    "headers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('./Dataset/May01_09_new.pkl')\n",
    "df.set_index('DateTime', inplace=True)\n",
    "del df['Second']\n",
    "del df['Minute']\n",
    "del df['Hour']\n",
    "del df['Day']\n",
    "del df['weekday']\n",
    "del df['YYYY']\n",
    "del df['MM']\n",
    "del df['DD']\n",
    "df = df.resample(\"5s\").agg({'Length':'sum'})\n",
    "df['DateTime'] = df.index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>DateTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2926</td>\n",
       "      <td>2007-05-01 19:55:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2007-05-01 19:56:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2007-05-01 19:56:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2007-05-01 19:56:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1115701</td>\n",
       "      <td>2007-05-01 19:56:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Length            DateTime\n",
       "0     2926 2007-05-01 19:55:55\n",
       "1        0 2007-05-01 19:56:00\n",
       "2        0 2007-05-01 19:56:05\n",
       "3        0 2007-05-01 19:56:10\n",
       "4  1115701 2007-05-01 19:56:15"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = pd.read_pickle('./Dataset/May22_31_new.pkl')\n",
    "df_2.set_index('DateTime', inplace=True)\n",
    "del df_2['Second']\n",
    "del df_2['Minute']\n",
    "del df_2['Hour']\n",
    "del df_2['Day']\n",
    "del df_2['weekday']\n",
    "del df_2['YYYY']\n",
    "del df_2['MM']\n",
    "del df_2['DD']\n",
    "df_2 = df_2.resample(\"5s\").agg({'Length':'sum'})\n",
    "df_2['DateTime'] = df_2.index\n",
    "df_2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.read_pickle('./Dataset/May31_June12_new.pkl')\n",
    "df_3.set_index('DateTime', inplace=True)\n",
    "del df_3['Second']\n",
    "del df_3['Minute']\n",
    "del df_3['Hour']\n",
    "del df_3['Day']\n",
    "del df_3['weekday']\n",
    "del df_3['YYYY']\n",
    "del df_3['MM']\n",
    "del df_3['DD']\n",
    "df_3 = df_3.resample(\"5s\").agg({'Length':'sum'})\n",
    "df_3['DateTime'] = df_3.index\n",
    "df_3.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = pd.read_pickle('./Dataset/June15_16_new.pkl')\n",
    "df_4.set_index('DateTime', inplace=True)\n",
    "del df_4['Second']\n",
    "del df_4['Minute']\n",
    "del df_4['Hour']\n",
    "del df_4['Day']\n",
    "del df_4['weekday']\n",
    "del df_4['YYYY']\n",
    "del df_4['MM']\n",
    "del df_4['DD']\n",
    "df_4 = df_4.resample(\"5s\").agg({'Length':'sum'})\n",
    "df_4['DateTime'] = df_4.index\n",
    "df_4.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df.append(df_2, ignore_index=True)\n",
    "df_merged = df_merged.append(df_3, ignore_index=True)\n",
    "df_merged = df_merged.append(df_4, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>DateTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9737</th>\n",
       "      <td>211670</td>\n",
       "      <td>2007-06-01 09:15:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>372962</td>\n",
       "      <td>2007-06-01 09:15:20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9739</th>\n",
       "      <td>225034</td>\n",
       "      <td>2007-06-01 09:15:25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9740</th>\n",
       "      <td>193683</td>\n",
       "      <td>2007-06-01 09:15:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9741</th>\n",
       "      <td>18386</td>\n",
       "      <td>2007-06-01 09:15:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Length            DateTime\n",
       "9737  211670 2007-06-01 09:15:15\n",
       "9738  372962 2007-06-01 09:15:20\n",
       "9739  225034 2007-06-01 09:15:25\n",
       "9740  193683 2007-06-01 09:15:30\n",
       "9741   18386 2007-06-01 09:15:35"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.shape[0] == df.shape[0] + df_2.shape[0] + df_3.shape[0] + df_4.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['DateTime'] = df_merged.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['Second'] = df_merged['DateTime'].apply(lambda x : x.second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['Minute'] = df_merged['DateTime'].apply(lambda x : x.minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged['Hour'] = df_merged['DateTime'].apply(lambda x : x.hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[\"weekday\"]=df_merged.apply(lambda row: row[\"DateTime\"].weekday(),axis=1)\n",
    "df_merged[\"weekday\"] = (df_merged[\"weekday\"] < 5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={0:'Weekend',1:'Weekday'}\n",
    "df_merged['Day'] = df_merged.weekday.map(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8*24*60*12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_may22_31 = pd.read_pickle('./Dataset/May22_31_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(df_may22_31, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_may31_12 = pd.read_pickle('./Dataset/May31_June12_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(df_may31_12, ignore_index= True)\n",
    "del df_may31_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_june15_16 = pd.read_pickle('./Dataset/June15_16_new.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.append(df_june15_16, ignore_index= True)\n",
    "del df_june15_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Number of riws and columns:', df.shape)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./Dataset/Entire_Data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_hdf('Dataset/Concated_Data.hdf','mydata',mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['weekday']] = df[['weekday']].astype('B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Source']\n",
    "del df['Destination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('./Dataset/Entire_Data_NOSrcDest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of riws and column after removing missing values:',df.shape)\n",
    "print('The time series starts from: ',df.DateTime.min())\n",
    "print('The time series ends on: ',df.DateTime.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are several statistical tests that we can use toquantify whether our data look as though it was drawn from a Gaussian distibution. And we will\n",
    "####  use D'Agostino's K2 Test. In the SciPy implementation of the test, we will interpret the p value as follows. p <= alpha: reject H0, not normal. p > alpha: fail to reject H0, normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p = stats.normaltest(df.Length)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "alpha  = 0.05\n",
    "if p > alpha:\n",
    "    print('Data looks Gaussian (fail to reject Null hypothesis H0)')\n",
    "else:\n",
    "    print('Data does not look gaussian (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.Length)\n",
    "print('Kurtosis of normal distribution: {}'.format(stats.kurtosis(df.Length)))\n",
    "print('Skewness of normal distribution: {}'.format(stats.skew(df.Length)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Time Series plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.loc[:, ['DateTime', 'Length']]\n",
    "df1.set_index('DateTime', inplace = True)\n",
    "df1.plot(figsize=(20,5))\n",
    "plt.ylabel('Packet Length')\n",
    "plt.legend().set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.title('Packet Header Length')\n",
    "sns.despine(top=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Box Plot of Daily vs Hourly packet length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "sns.boxplot(x='Day', y='Length', data=df)\n",
    "plt.xlabel('Day')\n",
    "plt.title('Box Plot of daily packet length')\n",
    "sns.despine(left=True)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.boxplot(x='Hour', y='Length', data=df)\n",
    "plt.xlabel('Hour')\n",
    "plt.title('Box plot of hourly packet length')\n",
    "sns.despine(left=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Active Power Distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(1,2,1)\n",
    "df['Length'].hist(bins=50)\n",
    "plt.title('Packet length distribution')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "stats.probplot(df['Length'], plot=plt)\n",
    "df1.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Packet Length Resampled Over Day, Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,16))\n",
    "fig.subplots_adjust(hspace=1)\n",
    "ax1 = fig.add_subplot(2,1,1)\n",
    "ax1.plot(df1['Length'].resample('D').mean(), linewidth=1)\n",
    "ax1.set_title('Mean Packet Length resampled over day')\n",
    "ax1.tick_params(axis='both', which='major')\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(2,1,2)\n",
    "ax2.plot(df1['Length'].resample('H').mean(), linewidth=1)\n",
    "ax2.set_title('Mean Packet Length resampled over hour')\n",
    "ax2.tick_params(axis='both', which='major')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Agg Packet Length Grouped by Day, Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,8))\n",
    "plt.subplot(2,2,1)\n",
    "df.groupby('Day').Length.agg('mean').plot()\n",
    "plt.xlabel('')\n",
    "plt.title('Mean oPacket length by day')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "df.groupby('Day').Length.agg('sum').plot()\n",
    "plt.xlabel('')\n",
    "plt.title('Sum Packet length by day')\n",
    "\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "df.groupby('Hour').Length.agg('mean').plot()\n",
    "plt.xlabel('')\n",
    "plt.title('Mean oPacket length by hour')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "df.groupby('Hour').Length.agg('sum').plot()\n",
    "plt.xlabel('')\n",
    "plt.title('Sum Packet length by hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(df, values='Length', columns='Day', index='Hour').plot(subplots = True, figsize=(12,12),  sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(df, values = 'Length', columns='Day', index='Hour')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationary:\n",
    "### In statistics, the Dickeyâ€“Fuller test tests the null hypothesis that a unit root is present in an autoregressive model. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity.\n",
    "### Stationary series has constant mean and variance over time. Rolling average and the rolling standard deviation of time series do not change over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dickey-Fuller test\n",
    "#### Null Hypothesis (H0): It suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.\n",
    "#### Alternate Hypothesis (H1): It suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.\n",
    "##### p-value > 0.05: Accept the null hypothesis (H0), the data has a unit root and is non-stationary.\n",
    "##### p-value <= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.resample('H', how=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(timeseries):\n",
    "    rolmean = timeseries.rolling(window=30).mean()\n",
    "    rolstd = timeseries.rolling(window=30).std()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(14,5))\n",
    "    sns.despine(left=True)\n",
    "    orig =plt.plot(timeseries, color='blue', label='Original')\n",
    "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    std = plt.plot(rolstd, color='black', label='Rolling Std')\n",
    "    \n",
    "    \n",
    "    plt.legend(loc='best'); plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    print('<Results of Dickey-Fuller test>')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistics', 'p-value', '#Lags Used', ' Number of Observations Used'])\n",
    "    \n",
    "    \n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical value (%s)' %key] = value\n",
    "    print(dfoutput)\n",
    "    \n",
    "test_stationarity(df2.Length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following data pre-processing and feature engineering need to be done before construct the LSTM model.\n",
    "##### Create the dataset, ensure all data is float.\n",
    "##### Normalize the features.\n",
    "##### Split into training and test sets.\n",
    "##### Convert an array of values into a dataset matrix.\n",
    "##### Reshape into X=t and Y=t+1.\n",
    "##### Reshape input to be 3D (num_samples, num_timesteps, num_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum_secs = pd.DataFrame(df.groupby(['YYYY','MM','DD','Hour','Minute','Second']).agg({'Length': 'sum'})).reset_index()\n",
    "df_sum_mins = pd.DataFrame(df.groupby(['YYYY','MM','DD','Hour','Minute']).agg({'Length': 'sum'})).reset_index()\n",
    "df_sum_hrs = pd.DataFrame(df.groupby(['YYYY','MM','DD','Hour']).agg({'Length': 'sum'})).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_sum_secs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum_secs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df_sum_secs.Length.values #numpy.ndarray\n",
    "dataset = dataset.astype('float32')\n",
    "dataset = np.reshape(dataset, (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "dataset = scaler.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataset, look_back=1):\n",
    "    X,Y = [] , []\n",
    "    \n",
    "    for i in range(len(dataset) - look_back -1):\n",
    "        a = dataset[i: (i+look_back), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + look_back, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "look_back = 60\n",
    "X_train, Y_train = create_dataset(train, look_back)\n",
    "X_test, Y_test = create_dataset(test, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "#### LSTM with 100 neurons\n",
    "#### Dropout 20%\n",
    "### Use the MSE loss function\n",
    "### 20 trainng epochs a batch size of 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "history = model.fit(X_train, Y_train, epochs=100, batch_size=70, validation_data=(X_test, Y_test),\n",
    "                   callbacks=[EarlyStopping(monitor='val_loss', patience=10)], verbose=1, shuffle=False)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "# invert predictions\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "Y_train = scaler.inverse_transform([Y_train])\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "Y_test = scaler.inverse_transform([Y_test])\n",
    "\n",
    "\n",
    "print('Train Mean Absolute Error:', mean_absolute_error(Y_train[0], train_predict[:,0]))\n",
    "print('Train Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_train[0], train_predict[:,0])))\n",
    "print('Test Mean Absolute Error:', mean_absolute_error(Y_test[0], test_predict[:,0]))\n",
    "print('Test Root Mean Squared Error:',np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot model loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Test Loss')\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Actual vs. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=[x for x in range(1000)]\n",
    "plt.figure(figsize=(16,4))\n",
    "plt.plot(aa, Y_test[0][1500:2500], marker='.', label=\"actual\")\n",
    "plt.plot(aa, test_predict[:,0][1500:2500], 'r', label=\"prediction\")\n",
    "# plt.tick_params(left=False, labelleft=True) #remove ticks\n",
    "plt.tight_layout()\n",
    "sns.despine(top=True)\n",
    "plt.subplots_adjust(left=0.07)\n",
    "plt.ylabel('Total Packer Length', size=15)\n",
    "plt.xlabel('Time step', size=15)\n",
    "plt.legend(fontsize=15)\n",
    "plt.show();\n",
    "plt.savefig('seconds_2st_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
